{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow Deep MNIST for Experts Tutorial\n",
    "(https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#deep-mnist-for-experts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "## Setup\n",
    "from constants import *\n",
    "from util import *\n",
    "from analysis import *\n",
    "from classifier import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# For testing their code\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14590, 1024)\n"
     ]
    }
   ],
   "source": [
    "x_total, y_total = preprocessing(pull=False, fine=False)\n",
    "x_total = np.concatenate((x_total, np.zeros((x_total.shape[0], (32 ** 2 ) - x_total.shape[1]), dtype = x_total.dtype)), axis=1)\n",
    "print x_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10942, 1024)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_total, y_total, test_size=TEST_SET_RATIO, random_state=42)\n",
    "\n",
    "max_x = max(x_train.max(), x_train.max())\n",
    "x_train *= 1.0 / max_x\n",
    "x_test *= 1.0 / max_x\n",
    "\n",
    "print x_train.shape\n",
    "\n",
    "\n",
    "y_train = boolean_y(y_train)\n",
    "y_test = boolean_y(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Start interactiveSession\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Building softmax regression\n",
    "x = tf.placeholder(\"float\", shape=[None, 32 ** 2])\n",
    "y_ = tf.placeholder(\"float\", shape=[None, NUM_CLASSES])\n",
    "\n",
    "W = tf.Variable(tf.zeros([32 ** 2, NUM_CLASSES]))\n",
    "b = tf.Variable(tf.zeros([NUM_CLASSES]))\n",
    "\n",
    "# define the outputs\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cost function\n",
    "cross_entropy = -tf.reduce_sum(y_ * tf.log(y + 1e-7))\n",
    "# cross_entropy = -tf.reduce_sum(y_*tf.log(tf.clip_by_value(y_conv,1e-10,1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1\n",
      "Epoch number: 2\n",
      "Epoch number: 3\n",
      "Epoch number: 4\n",
      "Epoch number: 5\n",
      "Epoch number: 6\n",
      "Epoch number: 7\n",
      "Epoch number: 8\n",
      "Epoch number: 9\n",
      "Epoch number: 10\n",
      "Epoch number: 11\n",
      "Epoch number: 12\n",
      "Epoch number: 13\n",
      "Epoch number: 14\n",
      "Epoch number: 15\n",
      "Epoch number: 16\n",
      "Epoch number: 17\n",
      "Epoch number: 18\n",
      "Epoch number: 19\n",
      "Epoch number: 20\n",
      "Epoch number: 21\n",
      "Epoch number: 22\n",
      "Epoch number: 23\n",
      "Epoch number: 24\n",
      "Epoch number: 25\n",
      "Epoch number: 26\n",
      "Epoch number: 27\n",
      "Epoch number: 28\n",
      "Epoch number: 29\n",
      "Epoch number: 30\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 30\n",
    "BATCH_SIZE = 10\n",
    "epoch_place = 0\n",
    "\n",
    "\n",
    "## train the model\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "## initialize all varables\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for j in xrange(NUM_EPOCHS):\n",
    "    print \"Epoch number:\", j + 1\n",
    "    num_batches = max(len(x_train) // BATCH_SIZE, 1)\n",
    "    batch_size = len(x_train) // num_batches\n",
    "    x_epoch, y_epoch = permute(x_train, y_train)\n",
    "    for i in xrange(num_batches):\n",
    "        train_step.run(feed_dict={x: x_epoch[i * batch_size : (i + 1) * batch_size], y_: y_epoch[i * batch_size : (i + 1) * batch_size]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 0.672149\n",
      "Train: 0.68068\n"
     ]
    }
   ],
   "source": [
    "## Evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "print 'Test:', accuracy.eval(feed_dict={x: x_test, y_: y_test})\n",
    "print 'Train:', accuracy.eval(feed_dict={x: x_train, y_: y_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Multilayer convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## convolution and pooling\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1., 1., 1., 1.], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## build the computation graph\n",
    "\n",
    "x = tf.placeholder(\"float\", shape=[None, 32 ** 2])\n",
    "y_ = tf.placeholder(\"float\", shape=[None, NUM_CLASSES])\n",
    "\n",
    "# Batchsize, width/height, color channels\n",
    "x_image = tf.reshape(x, [-1, 32, 32, 1])\n",
    "\n",
    "## 5x5 image patches, 1 color channel, 32 outputs\n",
    "\n",
    "W_conv0 = weight_variable([5, 5, 1, 32])\n",
    "b_conv0 = bias_variable([32])\n",
    "h_conv0 = tf.nn.relu(conv2d(x_image, W_conv0) + b_conv0)\n",
    "\n",
    "h_pool0 = max_pool_2x2(h_conv0)\n",
    "\n",
    "# First layer\n",
    "W_conv1a = weight_variable([5, 5, 32, 64])\n",
    "b_conv1a = bias_variable([64])\n",
    "h_conv1a = tf.nn.relu(conv2d(h_pool0, W_conv1a) + b_conv1a)\n",
    "\n",
    "# W_conv1b = weight_variable([5, 5, 32, 32])\n",
    "# b_conv1b = bias_variable([32])\n",
    "# h_conv1b = tf.nn.relu(conv2d(h_conv1a, W_conv1b) + b_conv1b)\n",
    "\n",
    "h_pool1 = max_pool_2x2(h_conv1a)\n",
    "\n",
    "# Second layer\n",
    "W_conv2a = weight_variable([5, 5, 64, 128])\n",
    "b_conv2a = bias_variable([128])\n",
    "h_conv2a = tf.nn.relu(conv2d(h_pool1, W_conv2a) + b_conv2a)\n",
    "\n",
    "# W_conv2b = weight_variable([5, 5, 64, 64])\n",
    "# b_conv2b = bias_variable([64])\n",
    "# h_conv2b = tf.nn.relu(conv2d(h_conv2a, W_conv2b) + b_conv2b)\n",
    "\n",
    "h_pool2 = max_pool_2x2(h_conv2a)\n",
    "\n",
    "# Third layer\n",
    "# W_conv3 = weight_variable([5, 5, 32, 64])\n",
    "# b_conv3 = bias_variable([64])\n",
    "\n",
    "# h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "# h_pool3 = max_pool_2x2(h_conv3)\n",
    "\n",
    "# W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "# b_conv2 = bias_variable([64])\n",
    "\n",
    "# h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "# h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\n",
    "## densely connected\n",
    "W_fc1 = weight_variable([4 * 4 * 128, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 4 * 4 * 128])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, NUM_CLASSES])\n",
    "b_fc2 = bias_variable([NUM_CLASSES])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1\n",
      "  Number of batches: 364\n",
      "     Step: 0, Training accuracy: 0.500000\n",
      "     Step: 50, Training accuracy: 0.366667\n",
      "     Step: 100, Training accuracy: 0.566667\n",
      "     Step: 150, Training accuracy: 0.433333\n",
      "     Step: 200, Training accuracy: 0.600000\n",
      "     Step: 250, Training accuracy: 0.700000\n",
      "     Step: 300, Training accuracy: 0.566667\n",
      "     Step: 350, Training accuracy: 0.466667\n",
      "Test accuracy 0.502467\n",
      "Epoch number: 2\n",
      "  Number of batches: 364\n",
      "     Step: 0, Training accuracy: 0.433333\n",
      "     Step: 50, Training accuracy: 0.433333\n",
      "     Step: 100, Training accuracy: 0.500000\n",
      "     Step: 150, Training accuracy: 0.566667\n",
      "     Step: 200, Training accuracy: 0.666667\n",
      "     Step: 250, Training accuracy: 0.466667\n",
      "     Step: 300, Training accuracy: 0.466667\n",
      "     Step: 350, Training accuracy: 0.366667\n",
      "Test accuracy 0.497533\n",
      "Epoch number: 3\n",
      "  Number of batches: 364\n",
      "     Step: 0, Training accuracy: 0.533333\n",
      "     Step: 50, Training accuracy: 0.400000\n",
      "     Step: 100, Training accuracy: 0.600000\n",
      "     Step: 150, Training accuracy: 0.433333\n",
      "     Step: 200, Training accuracy: 0.566667\n",
      "     Step: 250, Training accuracy: 0.433333\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-eb828a72df89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"     Step: %d, Training accuracy: %f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test accuracy %g\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \"\"\"\n\u001b[0;32m-> 1325\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   2943\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2944\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 2945\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 428\u001b[0;31m                                target_list)\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 20\n",
    "batch_size = 30\n",
    "\n",
    "## train and evaluate the model\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv + 1e-8))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for j in xrange(NUM_EPOCHS):\n",
    "    print \"Epoch number:\", j + 1\n",
    "    num_batches = max(x_train.shape[0] // batch_size, 1)\n",
    "    x_epoch, y_epoch = permute(x_train, y_train)\n",
    "    print \"  Number of batches:\", num_batches\n",
    "    for i in xrange(num_batches):\n",
    "        x_batch, y_batch = x_epoch[i * batch_size : (i + 1) * batch_size], y_epoch[i * batch_size : (i + 1) * batch_size]\n",
    "        assert(len(x_batch) != 0)\n",
    "        if i%50 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={ x:x_batch, y_: y_batch, keep_prob: 1.0})\n",
    "            print(\"     Step: %d, Training accuracy: %f\"%(i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: x_batch, y_: y_batch, keep_prob: 0.5})\n",
    "    print(\"Test accuracy %g\"%accuracy.eval(feed_dict={x: x_test, y_: y_test, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3648, 625) (10942, 2)\n"
     ]
    }
   ],
   "source": [
    "# print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: x_test, y_: y_test, keep_prob: 1.0}))\n",
    "print x_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
